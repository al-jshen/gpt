{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56e10411-6c0a-40df-99df-d38de22fc12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ddd85f9-e2a3-48f5-98ff-acb5c7188b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from gpt.model import ViT, Lambda, LightningWrapper\n",
    "from gpt.alt_model import ViT as AltViT\n",
    "from gpt.data import MNISTDataModule, CIFAR10DataModule\n",
    "from tqdm.auto import tqdm\n",
    "import lightning.pytorch as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from rich.progress import track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57a995ee-caff-47d1-8812-43aeb08c2539",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = MNISTDataModule(root_dir='/mnt/home/jshen/ceph/data/ml', num_workers=4, batch_size=8)\n",
    "mnist.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f1d79ef-5470-42f3-8dfb-e7bac78c04da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/jshen/miniconda3/envs/main/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "tl = mnist.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fea44718-6aa1-4f87-939f-e93a7e2a4903",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(tl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f999e44-bc68-4144-bb55-e3ac5b9701ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/jshen/miniconda3/envs/main/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'output_head' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['output_head'])`.\n"
     ]
    }
   ],
   "source": [
    "model = LightningWrapper.load_from_checkpoint('../train/lightning_logs/version_2984103/checkpoints/epoch=24-step=2950.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8174b65d-1837-4d5e-bc89-6fe8d541e38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(state_dict)\n",
    "# model.eval();\n",
    "model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bbfdfb3-adb8-4b0b-ae15-ec4e39050450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(imgs, **kwargs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = TF.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img), **kwargs)\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "062cf1d4-9d3b-48fb-be38-e55a05b660c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, tensor([4, 1, 1, 4, 0, 3, 3, 7]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgQAAABVCAYAAAA2T/ztAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/SrBM8AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAyUElEQVR4nO2d6W9bV3r/v5eX5OW+byIlUaIoa7Es27KdKGMndh1PZpzJTJIiTdAZdHmRon1foH9B3/btFOi8mQFaFJkURYKizWSyOBmPncWLEntkOVosyaIk7iJ5ycvt8t7fC+Gcn2Q7tmxL5uLzAYzEDKWcy3N4znOe5ftwqqqqYDAYDAaD8VSjafYAGAwGg8FgNB9mEDAYDAaDwWAGAYPBYDAYDGYQMBgMBoPBADMIGAwGg8FggBkEDAaDwWAwwAwCBoPBYDAYALQ7eZOiKFhbW4PVagXHcXs9JgaDwWAwGLuAqqoQRRHBYBAazf19ADsyCNbW1tDT07Mrg2MwGAwGg/FkWVlZQXd3933fs6OQgdVq3ZUBMRgMBoPBePLs5BzfkUHAwgQMBoPBYLQvOznHdxQyuNcvNplM4Hn+UX68bahUKqjVavTver0egiB0tIHUaDQgSRJIi4unYa5VVUW1WmVz/RTPtcFgaOKo9p6nda4rlQrq9Tp97WmYa1mWUS6X8Shtih7JIDCbzXj11VcRCAQe5cfbAlVVcfHiRXz55Zf0tbGxMZw6daqjD4l0Oo333nsP+XweAGCxWPDqq6/C7/c3eWR7h6qqOH/+PC5dukRfGx8fxwsvvNDRc51MJvHee+9BFEUAmy7F1157DV6vt8kj2ztUVcXnn3+OK1eu0NcOHTqEEydOdPRcx+NxvP/++ygWiwAAu92O1157DW63u8kj2ztUVcW5c+cwNTVFX5uYmMDx48ebOKq9Z319He+//z5KpdJD/+wjGQQ8zyMYDCIcDj/Kj7cFqqrixo0b216z2Wzo7+/v6I1DEATodDr6d61Wi1Ao1NFJpaqq4tq1a9tes9vtHT/XOp0OWq12299DoRBCoVATR7W3qKq67YAAAIfD0fFzzfP8Pee6q6uriaPaWxRFgc1m2/aa0+lEX19fR881gEf2/DAdAgaDwWAwGMwgYDAYDAaDwQwCBoPBYDAYYAYBg8FgMBgMMIOAwWAwGAwGHrHKgMHYaziOg6qq22ppOz0zmLFz2LpgMHYfZhAwWgatVguj0UjLwBKJBD7//HNwHAePx9PRIipPCxzHged58DwPvV5PX1cUBdVqFaqqotFoPPD3yLKMWCyGRqOB8fFx2Gw2ZLPZbYJDDAbj4WAGQQvztN2CeJ6H0WiE2+3G0NAQNBoN8vk8NBpNRwuoPA2QtazRaKDRaKDT6WA0Gum6rtfrkGV5R8YAsKm8l0wmIcsy9Ho97HY7RFFkBkELcC+FvDv3r6dtb2sXmEHQwqiqirW1NRSLRYyMjCAQCCCfz0OSpGYPbU8g8qrz8/NYWFiALMsYHh6GVquFzWaDoijI5/NQFKXZQ2XsECIVOzMzg6+//hqRSARHjx6FIAhwuVz0MFhdXcWlS5dgMBgQiUS2iejcC41GA5vNBkmScP78eQiCgLGxMfh8PuTzeVSr1SfxeIx7UCwWkc/nUSqVkM/nsX//fpw5c4a23l1bW8MXX3wBrVa7bQ0wmk/LGgTfZ0E+SJ+5kxaXqqpIJpNIJpMYHR2Fx+NBtVrtaIOgXC4jk8lgenoagUAAZ86coa7lcrkMURTb2iC4c/0+ynrdyQ2sVdDr9TCbzUgmk/jss89Qr9dx5MgR6HQ62Gw2Ou6VlRVMT0/D4XCgt7f3gQYB0eKXZRlTU1Oo1WoYHh6Gy+WCJEnMIHgCbO2LsBVJkpBKpZBOpxGLxRCJRHDgwIFtc5rNZqHX6+FwOKihwGg+LWsQVCoV5HI5uFwuRKNRVKtVpFIp5PN5zM7OQq/XU+nkfD4Pnufh9/u3ye62OxqNBvv376feAY7jWnbj3w0EQYDNZoNer0etVoPT6aQNWOr1ekdsHPV6HQsLC6jVaohEIg/VWpy42+fm5jA/Pw+n0wmPxwObzQan09kSa4PjOGg0GlgsFlgsFsTjcczNzcHhcODv/u7v0Nvbi2AwCIPBgFqthmq1ikKhAIPBgL/9279Fo9GALMsP/P/o9XqMjo5CVVUEg0GUy2WkUinkcjkYjcaO2gdaEVVVsbq6ilQqhWeeeQaHDx9GsViEKIpwuVzo7u6GJEkQRRH79u1DMpnc9v09cuQIjEYj+vv7IUkS5ubmdjTvjL2lZQ2CcrmMRCIBs9mM7u5uiKKITCaDQqGAq1evwmw2w+FwANi8Xeh0Orhcro7aCDQaDUZGRuB0OqGqalvfjHeCXq+H2+2GxWKBXq+HyWSCyWSiz94pBsF3330HURTh8/ke2iDQarVYXl7GJ598gkgkgqGhIYRCITidzj0c9c4hY3Q4HPD7/VhcXMTU1BQmJyfx1ltvUaNFlmVUKhWIooiVlRX09vbitddeQyqVwu9+97sH3vD1ej2Gh4dhsVgwNjaGUqmE//7v/8bKygrGx8fh8/mexOM+1cRiMczMzODEiRM4evQoEokE4vH4PT1YqVSK/ruqqjh8+DCsViv6+vqQTCaxtLTEDIIWoOUMApvNBq/XC1mWkUwmoaoqLly4QA8Do9GI3t5e2O12DAwMQJZlbGxsdOTtWVVVZLNZSJIEh8MBk8kEr9cLm81GjaNOgswhcSWS9sOP0saz1dBoNLDb7bDb7Th27Bjy+TwWFxexsLCA0dFRuFyuHf0OrVaLAwcOwG63Q6vVQqvVwmKxPIEnuD/EMxAIBOD1esHzPDiOQ3d3N44fP47u7m7U63Wsra3hu+++g91uRygUQqVSAcdxEEUR165dQ6lU2nGVQSKRwMbGBm33msvlUCqVYLVa4fP5UCgUUKlUdu0ZjUYjPB4PKpUK1tfXIUkSEokEDAYDhoaGoCgKlpaWUKvVwHEcDAYDxsfHYTKZsLq6uqtjaSZkLzp27Bh6e3vh8/mQSqVod73v24eJB7BWq6FQKKBeryOdTmN1dRXT09NQFAXhcLglL3XVahUbGxsANp/DYDCgt7f3nmOtVqt0fZI5J9+Fe10ASEJtuVxu+hppOYPA4XBgZGQEyWQSa2trSKVSSCQS8Hq9OHjwIEwmE6LRKFwuF0ZHRyFJEm7dugVZljviBrkVkkOgKAoGBwdhsVgQCARgNBoxMzPTcQYBAJqBbjabO8rI43kePp8PgiDA4/FgY2MD//Iv/4LFxUV4vd4dGwQ8z+OZZ57Byy+/jMXFRRo+azaklLCnpwfDw8NIp9NIp9Po7+9Hf38/6vU6qtUqZmdn8e6772Lfvn344Q9/CGBzs8zn8/j66693/P9rNBqIxWL056vVKjWSbTYburu7sbi4uKsbrMlkwsDAALLZLFZWVpDL5TA9PQ2n04mJiQk0Gg2srq6iUChAo9HA6XTilVdegdfrRS6Xa/pmv1u43W74/X709fXR19bW1h74c0ajEV1dXRBFEaIoolqtIh6PY2lpCVevXoUgCOjq6mpZg2B1dRXA5qXV7Xajq6vrLmNcURSIoohSqYSlpSVks1kAmyXVY2Nj9+waWyqVaMir2Wuk5QwCURSxuLgIjUaDF154gd4WtVrttjr0TjkoHkSj0UCj0YCiKB1xU34YOmmOG40G1tbWoNfr4XQ6Ybfbcfz4cQwODsLr9e7od5BSvVbaMIlnwOfzwel0QhAE5HI5lMtlAEAikcDq6ipMJhOsVisMBgNGRkbg8/l2XGL4IFRVhVarxcGDB1GtVlGtVjE3N0fH8LCQ75wkSdjY2IDVakVXVxc4jkOhUIAsy+jq6oJer0cikYDNZqMG7NDQEMrlMnieh8lkokmywGaYo9FoQFVVSJK0zUXu8/lgsVggiuIjj/tJcr/v5srKChYWFmAwGGA2m2EwGGCxWBCLxXDlyhWYTCb4/X56gXO73XjllVfQaDRgNBqf1CPsCI1GA71eD61Wi/3796NQKOBPf/oTPfTvTH6VZRlra2vI5/OYn59HPp/H2NgYvF4vtFotPfBVVcXNmzexvr6OaDSKUCgEo9GIcrkMWZabFj5pOYMgk8kgnU7D6/XiL//yL+liSqVSuHr1alt8WXYTWZZRr9efWqOgU6jValhfXwfP83juuefg9XrxxhtvoF6vY2VlBcVi8YG/g2Tst5JBQDwD/f39iEajSKVSiMfj9L/funUL586dQ39/P0ZHR2GxWPCDH/wAHMftmkEAbH42L774IoxGI/7whz9gdnYWwWAQNpvtoX9Xo9FAtVpFMpnEzMwMwuEw+vv7AWzGwgVBQDQahdfrRalUgsFggN1uh16vx7PPPksNFFVVUSwWqavZZDJRd3KpVKLVQhqNBtFoFL29vVhaWmr7PW5mZgb/9V//BbfbjVAoBLfbjZ6eHiwvL+OPf/wj9u/fj7fffhsGgwEAaFgpm83iww8/pKGHVoDneVgsFphMJgQCAczPz+Pdd9+FxWLBmTNn7jIIKpUK5ufnkUgkcPXqVRSLRTz//PMYGRkBAPpsjUYDH374Ic6fP49/+Id/wL59+1Aul1Gv11EqlZhBwPM8tFot4vE4FhYWMDQ0hMHBQapc1ukJdfdCVVUUCgWUSiWaZHXjxg2k0+mnxjBo97ABz/NwOp2o1WpYXFykWfT1eh1TU1PIZrOw2WwQBGFHv498HhqNpumfC/EMkMOwUChQtcFEIoFkMonZ2Vncvn0bNpsNsixDVdVdDe2R74GqqlQFsaurC4IgPLKyZS6Xw9LSEgwGAw4ePAitVotbt27RsGWtVsN3332HUqmESqUCVVURj8fBcRzS6TQURYHBYKBhDVVVcfToUTidTjr3iqIgl8vBZDJBp9NRoaXZ2VksLi7SeQ4EAvD7/bv2ee0GoihCo9EgnU4jl8vR110uF1wuF3ieRyQSgcVioZVCABAIBPDSSy8hEAhsO0jJem7FkK/BYEBXVxdqtRpmZmaQTCYRjUapcZfP57e9v16vU6GswcFBVCoVaDSau8K7iqIgFArh6NGjdH7tdjtMJhPi8XjTSstbxiAgceN4PI73338fp0+fxqlTp6ibpVarPTWHIIHkEKRSKYyNjQEAfve73+Hjjz/G6dOncezYsSaPcO/RaDRtPe96vR79/f0ol8v44IMPIIoiKpUKyuUyfvvb32J2dhY///nPMTg4uOPfSQ6LZhoEJJ8hGo1iYGAA8Xh8W4b57OwsLly4gNnZWczMzMBut+Po0aN7MmZFUaj3jOd5jI6OotFoYHFxkcZwH4ZYLIaPP/4YJ0+exF//9V/j5s2bePfddzE2Noaf/vSnWFlZwaeffgpZluHxeKDX63Hz5k2aI1Gv12ny3OXLl6HT6fDDH/4QR44cAQB6+4vH4wgGg7BYLJifn8d3332Hzz77DFevXqWelz/7sz+Dz+druvG3FaKNcunSJczMzNDXDx48iEOHDkEQBOoFAv5/eGF0dBSnTp2CJElYXV1ti0uexWLByMgIFhcX8Zvf/AYcx+H555+HIAgolUr39GZYLBaYzWYEAgEAm9+VrV4zYPMzGR8fx7Fjx+D3+6nxZ7fboSjKXe9/UrSMQZBIJJDNZlEul/Hcc88hGo1S1121WkW5XG7rg+FR4DgOwWAQTqeTZqd6vV4MDAzQkstOgujZm0wmOBwOqKraMQIzOp0O0WiUbiKyLNNb1KO4tYEHi3TtJVqtFjqdDvV6HcVikd56y+UydbfHYjGqINjV1bUn45BlGYuLi5BlGaFQCA6HY1cOT1VVkclkcPXqVeTzeQQCAXphyeVysNvtaDQaMJvNtNpDVVW4XC7IskxFk/r7+8HzPPL5PG7fvg1gc51zHAer1YpMJkO9KaIowu12Y2RkhMaoZVlGLpeDwWCAwWBoGcNAVVXY7XZ0d3fT16xWK302nufpvm21WuH1emmMnHiRCMViEdevX6frqJWoVCpYXV3FxsYGXVt35rPdyU6MdUVRcPv2bRSLRTidTthsNkQiEfT09NzldXiStIxBcO3aNfzP//wPzp49i3/6p3+CRqNBvV6HJEmQJOmpNAhIRrndbqc35fHxcQQCgY5s9FOv11EoFGAymRAOh1GpVLCystLsYT0WqqpClmUYDAa8/PLLKJfLmJqaQqlUwtmzZ2E2mx86u5joMjTz+0AOKFEUqVsc2FSgSyaTuHHjBq5evYpTp07hxz/+8Z6t10qlgt///vfIZDIYHx/fdkA9KmTTn5ubw7/+679icHAQZ86cgU6nw9WrV6GqKiKRCH0v+Sc5JLdCPD+3b9+mVREajQb9/f3w+/348MMPMT8/D4PBAL1ej7GxMTz//PP49ttvMT8/Tz0dgUBgz4yqR2VgYIDmVgC4y+UviiJWV1cxNDSEaDQKrVZLv89b1248HsfHH38MAOjp6WmpHJlcLocvv/wSqqrSudyN0IaiKDh37hwuX74Mk8kEg8GA06dPY3Jysqmek5YxCCwWC7q6uuB2u2E0GlEsFrG+vg4A1Dh4WlAUBclkEpVKBX6/H4IgULeoVqttiTKzvYBk4BuNRpRKpW1hInILa5Ub0oMgan06nY7Ww5OM60AggHK5TGPHDzrYifu4Vqshl8tBo9E0PRub3ILubFFNFEbNZjMGBwfh8/mg1+v3dN4ajQbq9TpVPnzcA8VqtaK/vx96vR5WqxWhUAhWq5VWBwC4p4Fzr2ck7yPfX2BzbZRKJZpr4HA4kM1mkU6naRJeOBwGz/Ow2+2wWq0oFou4efMmgsEgwuEwarVaUyXMt3atvBPi1dVqtQiFQjSv4E5NEZJAJ0kSnbNW+35v7b6520atzWaD3++Hy+WCzWaD3W5vujhTyxgEkUiEig1xHId4PI5z585RV0q7x5Ifhnq9josXLyIWi6G3t5dKNHc6drsd+/fvR61Ww/LyMk0U02g0MJvN9O/tAGnSoygKbty4AUVRMDo6CpvNhhMnTkBVVXz99deIx+MPvBEYjUYYjUYkk0lks1kMDg62jDLhnWSzWSwtLaG/vx+HDx9+YE+C3YAcTJIkUR2CxzEK+vr60N3dDbfbjb6+Ptpjg/x53H1IURSsra1RIadAIIB33nkHU1NTOHToEPr6+tDb2wtFUWC1WmGxWPCrX/0Kv/71r/HWW2/hRz/6EZLJJG7duvVY49grCoUC1tfXMTw8jMnJye815Ekjs3q9jmAwCGB3bt/tgEajwdjYGLq7uzE8PIze3l7kcrkdVRvtJU03CHK5HLLZLE3CIF3tdDodvS3eeQvpdDQaDRwOB2q1GgRBoM/fqZ+BVquFIAio1+uYn5+nGcfkD7lJlMvltkhEIpBKAJPJRKWXZVmmanbFYnFHz2M2m+F0Omn5KSnXMhgMVNb6SUPmjOd5qKqKer2Oer0Oi8WC3t5e6PV66HS6PTfgSFUB0WcglQyPA/FGabVaKIpCPRC75aUknxeBiBgRBVZiRJEbtSRJsFgsiEajMJvNyGQyTT84CKqq0rwYv9+PQCAAjUaDRqMBh8Nx37yHarVKvcBPU5Mj4i0hMu21Wg1LS0stUWnRdIPg2rVr+Oijj/Dyyy/jb/7mb6DValGr1eD1evHSSy/R0EGnHob3QqvV4tSpU+A4Dj09PdTd2Oy48V5htVrR09ODmZkZ/OY3v8HAwABef/11evARFTtFUeB0Otsif4LkDuj1ekSjUepiLRQK+OUvf4n19XW8+OKLNBP5fvT09GBoaAiJRAKZTIY+v9/vh9frRTKZfKK5FhzHwW63w+Vy0Q1sY2MD+Xweo6OjePnll3H9+nVMT0/v+VgMBgPOnj0LVVXhdrtRLpcfOUnzTkjJLwBquO3W969Wq0GWZWSzWTQaDTz77LM4c+bMthwIVVWxsrJCS93++Z//Gaurq/jkk09oSV8reMwWFhZw48YN/OIXv8DZs2epyuyDvEPZbBbnz5+H2WzGiRMn6Pe90zGZTBAEAZFIBGazGf/2b/+G//u//8OZM2dw/Pjxpo6t6QaB1Wql7jmDwQBJkhCPx2m99m5Y/O0G6fVONLM7HVJyarFYqJrd1o2OqIU1Go2W2AAfBLk1ZzIZCIJAY6hkPZtMJhobtlgs1NCr1Wr3FOvheR46nQ56vR56vZ52lSOf2ZP+TFRVxcbGBqrVKlwuF82or1QqNH68m6JD90Oj0cBkMtHQ0m7uFcSo2204jqPKhpIkoV6vw+VywW63o1wuIxaLwWg00u++0WiExWKB3W6nGiStsCeazWYYjUZ4vV74fD6YzWYAm99noj1wP/R6PTweD61CIAZzO3kBHwWyVvV6PQwGA3ieb5lnbrpBcPr0abz55ps0a/n69et47733kM/nkUgkEI1G8corr7RU5uleo9FoEA6H4Xa76U3izpreToJonJtMJrjdblrSRrBYLDh48CAqlQo1FluVrZK1n3/+OQwGA86cOQOLxYKNjQ3U63W89dZbVLBIp9OhWq2iVqthZWXlnv0pFEWhZX0A8Kc//Qlffvkl9u/fj8OHDz/x5LJGo4HPPvsMqVQKr7zyCpUMLpVKmJ6exuzsLGq12hMdUzvB8zyGhobgcrmoOh1xI3/wwQe4evUq9u/fj0gkgmg0irGxMZp0KAgCgsFg013LwKauwMjICA4cOID19XVYLBYsLi7u+HDr6enB22+/TcMh+Xwe586da3ulxoelu7sbhw4daokOnU0zCMiN585e7lqtlnoKgM3Np1gs0gOiVqtBp9PR+CTHcRAEgX5BiOXc7genTqeDwWCgHpJCodAS3bD2AtLF786YOOkARmqb22FOSckaWaMkoarRaCCbzaJWq9F4KakYIYenIAi0VpuU3NZqNcTjcVgsFhp3LpVKSCQStFtgMypw6vU6yuUyNc5qtRpKpRJVamPcn62KkzzPU++GLMt0PZDqDPJatVql+VWtAPFcWSwWuN1uAHiotUha1huNRur9MhqNUBSlo4XoSK7AxsYGcrkcVFWFx+NpeuUQ0ESDoKurC+FwGNlsFsvLy7DZbHC5XBgZGUF3dzfS6TSWlpaQz+dx/fp1ujhsNhu1rnmehyAI6O7upgI29Xq9rcrTHgS5HV64cAEzMzOIRqMtV4+8W5A4O5nrVCqFCxcu0PIlkqzUypCKCIfDgZ6eHqqFLooiPv/8cySTSdqf42c/+xm6urqQSCRQLpcRCAQQDocxNTWF1dVVXL58Gbdu3cKlS5fQ29uLZ555BgcOHEA2m8X8/Dw8Hg+tZHiScBwHv98Ps9lMy/FSqRQWFhbQ1dXVEjedVoZUGWSzWRouMhqN0Gq18Pv9OHz4MI4dO4bR0VHEYjFcvHiRhlBbaf1nMhkqCvU4VCoV3L59G41GA/39/ZAkCQsLCx3pZSIXWKPRiPfeew9XrlzBxMQEJiYmWsKQbppBoNfrYTKZaFtQvV6PSqUCrVYLn89HM4Z1Oh1tAQxs3qKsVuu2uCGp9ya36U60LCuVSksqee0l9Xod2WwWOp0Obre7ZW5G90NRFJRKJRiNRrhcrnuOeWuSKHm/KIoIBoMwGo0QBAF6vZ5WKZDbONl4ZVmmsfp6vd4U45dk4RPPXLVahSiK8Hg8T3ws7QZR4CS6IhqNBpVKhc4jyR0yGo3gOA6VSmWbjkGrkM/naQ7HTnIGqtUqisXiXfuzTqeD1WqFRqOBwWCAqqqwWq1U4rvVnvtRMRgM1BOq0+moMivP8zT/otk0zSDYWs5D3KIrKyvw+Xzo7++HzWZDNBpFX18fxsfHtwnUGI1G2gyJdFojNeqkhKNTXOvEtRwMBqm7uVMhIiBkriuVCjKZDHUjtgOZTAafffYZ7HY7/uIv/oLOl8Viweuvv04PcJ7n4XA40Gg0MD8/j3g8Dq/XC6vVimg0ip6eHgwODtK6erPZfJcKXjMplUrI5/P0FlcoFBCPx5l3YIeQ/ix9fX2w2+2Ynp7G2toaisUiFXcixnAoFEI+n9/WSKjZqKqK8+fPY2VlBS+++CJOnDjxwJ+Zm5vD+++/f5dHobe3F2+++SY9FK1WK06dOoVyuYzz589DFMU9eYYnCc/zGBsbg9/vp/1MfvKTn2BycrKlPD9NMwhIn4JSqUTV14gaW6FQoNYy2TgJ5H1bb0U8z28zLtrl8HgYBEGguulPC0Sit53mU1EUSJJ0l34E0a7f+izklk9a4oqiiFwuR7O0nU4nzUQmdf2tAvFSkI2MGK7k+ysIwo47OD4OpH8CAJq5X6lUqIu9VSHrgBiHpEpDlmUaJiRqf1arFaIoQpIkmp/SCiFRIkCUyWS26e9v1RDZWiJcKpWQTqfvmhez2Yx8Pg9VVWEymcDzPKxWK3iep96xrXli7QrZw8n33Waz0VyCVmn53LTThTQJuXjxIs6dO4dXX30Vf//3f48rV67g17/+Nb0lOhwOqlQIbC6e7u5uCIIAi8VCM5uTySTOnz8PYLNyoVWV3B6WTtYfeBB6vR4ul4u6z9sBj8eDN954A0ajkaorEsP30qVLEEWRlhnFYjHIsoxnnnkGw8PD+Prrr/G///u/OHDgAO29nkgkaALaD37wAxw+fLjZjwhVVSGKIjKZDKrVKjiOw/79++FwOHDx4kX8/ve/x3PPPYfJyck9P7gqlQo++OAD5HI5/NVf/RVsNhsuXryIVCqFUCjUUl6VOyFrIB6Pg+d59PT00MOh0WhgY2MDPT09CAaDmJ+fxyeffIJIJILR0dFmDx0A4Ha70d/fjxs3bmzTnHA6nfB4PHA4HNs6NTYaDZw8efKuvUySJPz7v/87vF4v3njjDfA8j1gshnQ6jfPnz6NQKODo0aMtPZcPQlEULC4uIpVKwWq1Qq/X49tvv8XNmzcRCoVaJtTWNIOA3IwymQySySStxeU4Duvr69SCrtVqNIEQ2FxUpVIJqqrCbDbTL04ymaRqT52QjFIulyGKIs2LIB6Qdrot7xTyfLVajR6YRIuAxKrbBZ1OB6/XS1X8CGSdbmxsQKvVotFoYG1tDY1GA4IgwOl0olgsIhaLIRQKwWw2Y2NjA5lMhpaekhbYrQDJ3yGbPdmsVVXF+vr6E1PSUxQF6XSa3jx5nkcmk0EsFoPT6Wz5Q4R4N0jeAFHsJH0r/H4/9SCQ7ock/tzsvcBgMMBmsyEejyOVStHXSYI3SZbcahR6vd67fk8ikcDq6ir16qiqinK5jGKxiHQ6TffBdkZVVeTzeZTLZapDkc/nkU6n4XK5mj08StN3WtIaNRAI4NNPP0U2m8Vzzz1HrUi9Xk9dgcCmq3J6epr2VydliVqtFj/96U+h0+naPs5erVbx29/+FoVCAS+//DIGBgawsrKCb7/9FmNjY7BYLM0e4q5SLBZx+/ZtXL9+He+++y6i0SjefPNNmlRI5KvbgUKhgOXlZVgsFkxOTtKSLLPZjEgkgnw+TyVpg8Egfa5kMonBwUF6q9XpdIhEIggGg/jmm29w69atpjaz2QrP8zh69Cg4jkNvby+ATYPAbDbjyJEj4DhuWxe8vUSr1WJwcBDBYJBKn9fr9bY7ROr1Oj3srVYrpqamcOvWLQwODlJFu7fffpvevtPpdNM7gbpcLlitVprfRCAlhDvNmjcYDOjp6YHH42kr4/9hUBQFCwsLkCQJdrudhgEPHTq0o4TMJ0XTP32Xy0UtpLW1NQCbkqzfB5H75DhuWxxOEASEQqGWKN14HMita3l5Gaurq3juuedoNurWTPNOgjSPSafTmJ2dhcFgoEmnJAu3XQyCWq2G1dVV2Gw2VKtVmEwm6HQ6mjm9la2310qlAofDsc2YtdlstFHP1ng9yTFo5lr3eDywWCzUWCeVEX6/nyYFy7JMY8m7HTogNfuyLMPhcMBsNsNsNtPbc7t508hYSaOmfD6P27dvU+/A4cOHMTw8TPVbmt3LgOM4KiZ357p+WEilwp0KpWRd1Wq1tgkZ3guiM1Eul5HP5yHLMlUwbbXLa9MNgsfF5XLh1VdfRblcxsLCAnVXtSM8zyMcDkMQBKysrMBisSCfz+PmzZt44YUXcPbsWSQSiW0JPJ0AyQs5fvw4lfVtV8NOFEV89dVXtOzQ6/Wiu7sbGo3modcmOeCI0UykbH/84x/jxIkTKBQK2NjY2IvHeCBELOfOQ7enpwdWqxU3btzAxYsX0dPTsyfeApKBLkkSnn32Wfj9/pYPD+wEUlL9ox/9CMePH6cZ6F6vF1qtFmtra1haWqLvb4XkwselWq1iZWXlLulum82GN954A+VymZZpthtarZbm10QiEYiiSJuUlcvllgtvt71BYDAYEAwGUSqVsLKy0tYGAcdxtMTM6/VCkiRUKhUkk0kcOHAAQ0ND+OqrrzrOINBqtbBYLNTDs9Ub0G4JlbVaDYlEAoIgYH19HY1Gg4rOPGxpEcncNxqNNBEJ2GwV7vf78e233+LSpUt78RgPhHhw7pwbm80Gk8mEGzduIB6Pw+Fw7OqhRfJNqtUqbt++jVKphDNnziAYDEKr1VKvRLuIk5GEUVJdQObb6XRCEASkUilkMhmYzWZoNBqIoojl5WVYrdaWij0/Doqi0D4YW9eTwWDAyMgIKpUKbt682TKZ+A8Dx3E0uVKr1dLcsEqlgkQiwQyC3aYdS9O+D1mWcePGDfA8j2AwiK6uLly4cAFTU1MQBAHpdHpb8k6nkM/n8d1331EpVBJjL5VKmJubg9PpxMTExBMpY3tcPB4PXn/9dWg0GoRCIRgMBlQqFWg0modao4qi4NKlS4jH4xgeHsabb74Jj8eDYrFIk8+aqbVBQhgkbk9uO0RoZ3x8nPal2NpG+HGRJAlffPEFRFFEd3c37ZRps9nwhz/8Ael0GtFoFBMTEy3fGI3jOAwMDMBut+Odd97BlStXEI1GEQwGMTY2hkgkApfLRY2BXC6Her3e9HDRbtPb24t//Md/3JXwQ6tRLpfxy1/+EqVSCT/5yU8wODiIarUKSZJaMvzb9gYBsBmDvtdtpd1QFAXxeByqquLQoUNwOp348MMPsbCwgL6+vo7aBLZSLpeRSCRgNpu3ld/UajWk02lqILQDZrMZBw8e3Pbao3zxSevbmZkZHD58mDYxkiQJ5XIZpVKpqbcLEqMnXduI9jypqw+FQvD5fLQpz+POH9FzqFQqmJubQ6lUwvPPP49gMEjDKQsLC1heXsaRI0cwNjaGubk5ZDKZXXrix4d8BuRz0mg0cLvd8Pv9yGazuHbtGlRVRb1eRzgcpo2yTCYTSqUSSqUSFEWBXq9vyRbgWzU3gLvDGVvXAHkfx3FwOp04ffo0VFXF0tISVWMlv6/dvIRbqdfr+OMf/4jbt2/j0KFDiEQiLa2X0/YGQTKZxPvvv49arUZvJO3M1haurbhg9gKSIEYSC/V6PSwWC/x+P1599VXo9fq28A7sJjzP45VXXsFrr72GYDAIYHNzqVQqWFtbw/r6+j07Iz4piOt+dXUV+XyeKuz5/X64XC5qKBQKBdp3IRgMUk2Gh6VYLOLSpUsolUoIh8Mwm82YnJyEz+ejIjaHDx9Gb28vZFnGrVu3mp54dyeNRoOGOUwmEwRBgMPhQD6fx+TkJPUIWK1WdHV10VK1QqFA+2OUy2Wq1NpKqKqKWCyG27dvIxgMoq+v7673SJKEbDYLSZKQyWQQCARw8uRJOBwOJBIJANjmRRJFEZ9++ikkSUIgEGjLPUCr1WJ8fBzBYBBfffUVrl+/jn379sHr9bakDH3bnZ5brWxg09388ccfQ1VVvPTSSy2XtfmwEKt6a0+Gdun096iQZyX19kTUx26349ixYy3pWttrOI7DxMQEBgYGkEqlkMvlqFaDJElNj6eSOctms8jlciiVSqhUKjCbzXC73fTAIrFSq9UKq9UKjuMeauzkO1Aul3Hjxg1Uq1W88MILCAaD2LdvH9xuNw1H9Pf3w+fzoVgsIplM7slzPw6KoiCZTCKbzcLpdMJkMtE8k3379uHYsWPb3k/CZplMhqrckV4XrUg2m8Xc3Bz0ej36+vruutXXajW6XpaXl8HzPC3JvVdeVLlcxpUrVyCKIk6fPt2yz30/eJ5HX18fXC4XLl68iPX1ddqPpxVpO4OgWCzi2rVrCAaDeP755+F2u/Hzn/+cKqZ1ChzHQafT4aWXXsKRI0da7kawm9jtdupKI5oSpGUwaYZyZ0lSp6OqKqanp7G4uEhvja0ICdVZrVY4nU64XC5YLBbUajXUajWEQiEcP34cVqsVBoMBoiiiWCyiq6sLY2Nj2NjYwOzs7Pd6w6rVKlZXV1Gr1XDy5EkIgoCRkRHY7XaqT7HVoGylbHSNRgO73U7laTmOw+DgIGRZhs/ng9lshsvlgiAIuHnzJtLpNK2WWFpawtraGkZGRrBv3z7UajUsLi62nNeDwHEcRkZGEI1G4fF44Pf7MT8/j6mpKWoY+Hw+PPvss5BlGYVCgaqQ3olWq4Xb7YYgCDh8+DDy+XxL1ervBK1Wi4GBAZjNZsTjcRSLRXg8HuTz+XuKM7UKbWcQSJKEubk5KuTh8/lw9uxZlEolfPnll02/Oe0WRKVvcnISgiDgm2++QSwWa/aw9gSLxYJwOIxSqYRkMkk3+EajAUmSaM3z04SqqtTtPTEx0bIGAZknh8NBdRNIM6pqtQqfzwe/3496vU5dpJIkwWQyYXx8HMvLy5ifn//eQ7xWqyEWi0EQBLzwwgtwuVzo6uqiBwn5ORJ7bzWDwGazUV0BYDOBzmAwoKurizbzaTQaWF5extWrVzE6Ooru7m58+eWX+Pbbb+FyuXDy5EmqS9LKRCIR9Pb2UsN9dnYW09PT1CAwmUw4cODA9+ZCbW1g53K5YDQaMTw83LTS2seBeAY8Hg9cLhckSUI4HEalUkE8Hm/Zhk1tZxD4fD68/vrrNJGo0WhQSchW2Qgeh62eALLJAZ2dTyBJEnWlkRLStbU11Ot12g/gafIOEEisuB1cpZIkodFo0Bav8XgciUQCbrcbPp8PGo2GtrEeGhqCxWLBwsICstksFaK6F3q9HpOTk9DpdDSufuHCBciyjKGhIZjNZno5IG2CWykBjYT7CoUCyuUyRkdH4fF4sLy8TN3miqJgdXUVmUwG9XodOp0OAwMDVPhpaWmppTodfh8WiwVdXV20nXd/fz9+9rOf0fkIhUL39XTmcjl8/fXX0Ol0mJiYoGI+7YgkSfiP//gPqKqKyclJBAIBuN1uAJtqpswg2CW8Xi8mJiZoF7hGo0G/bO1+aJIMbQJRXCO3sE6FVBkEAgFEo1GkUiksLCygXq+3Vevj3cZoNFKJ01anXC7ThDetVotYLIaFhQXs27ePGgQajYaKLFUqFTrHBoPhew9xQRAwOjpKW52n02lcvHgRhUIBJpOJ6nWQOv5WMgYAUIU9URRRLpcRDAYRDofxzTffYHZ2ls7t2traNoMgEokgEokAwDYholaGJAKnUimIoohwOIxwOLztPfcz7HO5HD766CPaxr5VvWI7oVwu4z//8z+RTCYRCATQ1dVFvR6tPJ9tZxAQSGZ6J6DRaODz+aDX61GtVlGr1XD58mXk83kcPHgQgUCgY0Ih94IkG0mShGQyiXg8jkuXLlE56k7On+g0SM5Ho9GgG/q9brdE6ZAYut93UDQaDYiiSA2KarWKwcFBlMtl8DxPWx1vTcJtFRRFQS6XA8dxVMJ6ZWUFxWIRsizT6ggANEFya3fAdiMej+Obb76hhhppUPQgisUibt26hY2NDUQiEfA83xZG8P3QarXYt28frY4olUpYWlqilSOtSlsbBPV6ndbztjNarRbd3d2w2+20n/yvfvUrfPXVV/jFL36BQ4cONXuIe0qlUkGlUoEoikgmk1hdXcUXX3yBnp4e/Pmf/zkzCNoIURTpAe5wOKCq6mNpAZCQIEFVVYyNjUFRFOqVaFUURUE2mwUA2pDs1q1b9L9vlVo+cODAkx3cHhCLxRCLxXDo0CHs27cP6XR6RwZBoVDApUuXaGKiTqd7AqPdW3Q6HcbGxiDLMgwGA4rF4hPtAvqotK1BsLWBCclobme3+tZEOp7n8dJLL2F0dBShUKjZQ3ti6HQ62Gw2KIpChZmeVmOA4zj4/X4qZd1ukNv6XtRaNxqNlgwP3I973frb1RPwINLpNKanp1EsFpHL5eBwOBAMBrG6uorLly+D53lYLBZaflcoFKjE9V40wnrSCIIAjuMQDofBcRz1/LZD6LOtDQKi1pbNZjvCGCD11FqtFm+//TbMZjM+++wzLC4uNnuITwRBECAIAjweDwYGBgB07qb5IIis7VYdgnZiaylgq+m1M/aWWCxGKyJUVcXw8DBCoRBu3ryJd955BwaDAeFwGHa7HX19feB5/r4dbtsJ0rlREARaDZNIJFAqldoixN12BoFer4fX60WxWMT09DTq9ToEQWjrsIGiKMhkMpAkiSYRajQaCILQ8i6m3eTO1qdPO4VCAYlEAlNTU5ifn6evk14G7DNitCpbvTf5fB6zs7MoFAoIh8PQ6XRUmInQKWtZURRsbGxAp9PB4/G0RYXQVtrOIDAajYhEIlheXsbly5fRaDQwMTEBo9HY7KE9MkTSdCskE7Ud3EyM3UdVVSpP/O677+Kjjz5Cb28vurq6MDQ0hKGhoWYPkcHYEaQEVVEUPPvsswA6V31VlmUqwxwOh2nuSLvQdgZBpVLBysoK0uk0HA4HFEXpiDjznfHQdoqPMvYG4mp3uVwYGhqC1+ulSoAMRruwtQKkE/bqB7E1f0aWZdjtdpjNZlpB1sq0nUGQy+Vw/vx5KIqCwcFBAGjrcAGD8X0Ui0VwHIexsTGMjIzQW1UnJF4xGJ2Mqqq0M2l/fz/t19DMhmQ7oe0Mgq36A0+Dtcl4ulFVFTzPs7XOYLQBRGadVL+JoohMJoNSqYRqtdrs4T2QtjMIGAwGg8FoRYgaJ/D/JYrX19cBoHOrDEgf9HZ4wMfhzqYa+XweCwsLHe2uTafT22rHZVlGLBZr+djX43JnWV8ul9smItOJpFKpbd/her2OWCyGSqXSxFHtLfdSinsa5jqRSNw11ysrK5AkqYmj2lvuNdcbGxsdP9ekpfajwKk7yF4rFArbVLU4joPRaOz42P2dNdQ6na7tykgeFqIAR5YFm+vO5Wmd62q1us3o1ev1bS+V+yDYXG/yNM41IZ/Pw2az3fdnH8kgYDAYDAaD0T7sxCDobPOQwWAwGAzGjtiRQcBq4hkMBoPBaF92co7vyCAQRfGxB8NgMBgMBqM57OQc31EOgaIoWFtbg9Vq7egMewaDwWAwOglVVSGKIoLB4AOTSHdkEDAYDAaDwehsWFIhg8FgMBgMZhAwGAwGg8FgBgGDwWAwGAwwg4DBYDAYDAaYQcBgMBgMBgPMIGAwGAwGgwFmEDAYDAaDwQDw/wClIG73nYpw8AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(torchvision.utils.make_grid(x)), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98bdd5e4-b03a-4cb0-9b1b-9a9474c6e7a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 1, 4, 4, 6, 2, 2, 7])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(model(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b15a59ab-aa81-42e4-9dd3-0d8948a598e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Working... <span style=\"color: #f92672; text-decoration-color: #f92672\">━╸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">  4%</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:22:59</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Working... \u001b[38;2;249;38;114m━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  4%\u001b[0m \u001b[36m0:22:59\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m track(tl, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(tl)):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 5\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m         pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(out, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(pred \u001b[38;5;241m==\u001b[39m y)\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ceph/users/jshen/programs/gpt/gpt/model.py:304\u001b[0m, in \u001b[0;36mLightningWrapper.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ceph/users/jshen/programs/gpt/gpt/model.py:257\u001b[0m, in \u001b[0;36mViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    254\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding)  \u001b[38;5;66;03m# (batch, n_patches, embed_dim)\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# pass through transformer blocks\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch, sequence_length, embed_dim)\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# normalize\u001b[39;00m\n\u001b[1;32m    260\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)  \u001b[38;5;66;03m# (batch, sequence_length, embed_dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ceph/users/jshen/programs/gpt/gpt/model.py:109\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    108\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattns(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x))\n\u001b[0;32m--> 109\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlps\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ceph/users/jshen/programs/gpt/gpt/model.py:82\u001b[0m, in \u001b[0;36mParallel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43m[\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfns\u001b[49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m/mnt/ceph/users/jshen/programs/gpt/gpt/model.py:82\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m([\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfns])\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ceph/users/jshen/programs/gpt/gpt/model.py:64\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "score = 0\n",
    "total = 0\n",
    "for x, y in track(tl, total=len(tl)):\n",
    "    with torch.no_grad():\n",
    "        out = model(x)\n",
    "        pred = torch.argmax(out, 1)\n",
    "    score += torch.sum(pred == y)\n",
    "    total += x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "f9a0409a-8289-47af-b30f-33e406b0ca3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9138)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a90e3ce-30eb-4210-976d-dc8b8e99d01a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
